üé§ –î–£–£–ù–î –°–≠–¢–ì–≠–õ –•”®–î–õ”®–õ –¢–ê–ù–ò–• –°–ò–°–¢–ï–ú - –¢”©—Å–ª–∏–π–Ω –ë–∏—á–∏–≥ –ë–∞—Ä–∏–º—Ç

1Ô∏è‚É£ –¢”©—Å–ª–∏–π–Ω —Ç–æ–π–º
–¢”©—Å–ª–∏–π–Ω –Ω—ç—Ä
–î—É—É —Ö–æ–æ–ª–æ–π–Ω —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª —Ç–∞–Ω–∏—Ö —Å–∏—Å—Ç–µ–º (Audio Emotion Recognition System)
–ó–æ—Ä–∏–ª–≥–æ
–ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä –±–∏—á—Å—ç–Ω –¥—É—É —Ö–æ–æ–ª–æ–π–≥ —à–∏–Ω–∂–∏–ª–∂, —Ö“Ø–Ω–∏–π —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª (–±–∞—è—Ä —Ö”©”©—Ä, –≥—É–Ω–∏–≥—Ç–∞–π, —É—É—Ä—Ç–∞–π –≥—ç—Ö –º—ç—Ç)-–∏–π–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Å–∏—Å—Ç–µ–º –±“Ø—Ç—ç—ç—Ö.
–•“Ø—Ä—ç—ç

–ë–æ–¥–∏—Ç —Ü–∞–≥ —Ö—É–≥–∞—Ü–∞–∞–Ω–¥ –¥—É—É –±–∏—á–∏—Ö
–î—É—É–Ω—ã —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–≥ –≥–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö (MFCC, Chroma, Spectral Features)
RandomForest –∑–∞–≥–≤–∞—Ä—ã–≥ –∞—à–∏–≥–ª–∞–Ω –∞–Ω–≥–∏–ª–∞—Ö
–í–∏–∑—É–∞–ª “Ø—Ä –¥“Ø–Ω —Ö–∞—Ä—É—É–ª–∞—Ö (Waveform, Spectrogram, MFCC)

–ê—à–∏–≥–ª–∞—Å–∞–Ω —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—É–¥

sounddevice: –ú–∏–∫—Ä–æ—Ñ–æ–Ω –∞—à–∏–≥–ª–∞–Ω –¥—É—É –±–∏—á–∏—Ö
librosa: –ê—É–¥–∏–æ —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö, —à–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö
scikit-learn: –ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º (RandomForest)
matplotlib/seaborn: –ì—Ä–∞—Ñ–∏–∫, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏
numpy: –ú–∞—Ç–µ–º–∞—Ç–∏–∫ —Ç–æ–æ—Ü–æ–æ–ª–æ–ª
joblib: –ó–∞–≥–≤–∞—Ä —Ö–∞–¥–≥–∞–ª–∞—Ö/–∞—á–∞–∞–ª–∞—Ö

–•“Ø–ª—ç—ç–≥–¥—ç–∂ –±—É–π “Ø—Ä –¥“Ø–Ω

80%+ –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª—Ç–∞–π —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª —Ç–∞–Ω–∏—Ö
–•—ç—Ä—ç–≥–ª—ç—Ö—ç–¥ —Ö—è–ª–±–∞—Ä –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥ ”©—Ä–≥”©–∂“Ø“Ø–ª—ç—Ö –±–æ–ª–æ–º–∂


2Ô∏è‚É£ –°–∏—Å—Ç–µ–º–∏–π–Ω –ó–∞–≥–≤–∞—Ä
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ú–∏–∫—Ä–æ—Ñ–æ–Ω (–î—É—É) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –î—É—É –±–∏—á–∏—Ö (sounddevice)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö (librosa)      ‚îÇ
‚îÇ - MFCC (40 –∫–æ—ç—Ñ)                 ‚îÇ
‚îÇ - Chroma (12)                    ‚îÇ
‚îÇ - Mel Spectrogram                ‚îÇ
‚îÇ - Spectral Contrast              ‚îÇ
‚îÇ - Tonnetz                        ‚îÇ
‚îÇ - Zero Crossing Rate             ‚îÇ
‚îÇ - Spectral Centroid/Rolloff      ‚îÇ
‚îÇ - RMS Energy                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–ª (StandardScaler)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RandomForest Classifier     ‚îÇ
‚îÇ (300 –º–æ–¥, max_depth=20)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ “Æ—Ä –¥“Ø–Ω                      ‚îÇ
‚îÇ - –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª ‚îÇ
‚îÇ - –ì—Ä–∞—Ñ–∏–∫ (4 —Ç”©—Ä”©–ª)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ì–æ–ª –±“Ø—Ä—ç–ª–¥—ç—Ö“Ø“Ø–Ω“Ø“Ø–¥

–î—É—É –±–∏—á–∏—Ö –º–æ–¥—É–ª—å (record_audio)

–ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä 3 —Å–µ–∫—É–Ω–¥—ã–Ω –¥—É—É –±–∏—á–∏—Ö
Sampling rate: 22050 Hz
–ú–æ–Ω–æ (1 —Å—É–≤–∞–≥)


–®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö –º–æ–¥—É–ª—å (extract_advanced_features)

9 —Ç”©—Ä–ª–∏–π–Ω —à–∏–Ω–∂ —á–∞–Ω–∞—Ä
–î—É–Ω–¥–∞–∂ –±–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç —Ö–∞–∑–∞–π–ª—Ç
–ù–∏–π—Ç ~200+ features


”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω (prepare_dataset)

Folder –±“Ø—Ä = 1 —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª
.wav —Ñ–∞–π–ª—É—É–¥
Label –∞–≤—Ç–æ–º–∞—Ç “Ø“Ø—Å–≥—ç—Ö


–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∑–∞–≥–≤–∞—Ä (train_model)

RandomForest (300 trees)
80/20 —Å—É—Ä–≥–∞–ª—Ç/—Ç–µ—Å—Ç —Ö—É–≤–∞–∞–ª—Ç
Class –±–∞–ª–∞–Ω—Å–∂—É—É–ª–∞–ª—Ç


–¢–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö (predict_emotion)

–ú–∞–≥–∞–¥–ª–∞–ª —Ç–æ–æ—Ü–æ—Ö
–•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä –º–∞–≥–∞–¥–ª–∞–ª—ã–≥ —Å–æ–Ω–≥–æ—Ö


–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏ (plot_emotion_results)

–ú–∞–≥–∞–¥–ª–∞–ª—ã–Ω –≥—Ä–∞—Ñ–∏–∫
Waveform
Mel Spectrogram
MFCC



”®–≥”©–≥–¥–ª–∏–π–Ω —É—Ä—Å–≥–∞–ª
–î—É—É –±–∏—á–ª—ç–≥ ‚Üí Trim (—á–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å—ç–≥ –∞—Ä–∏–ª–≥–∞—Ö) ‚Üí 
Feature extraction ‚Üí StandardScaler ‚Üí 
RandomForest ‚Üí –ú–∞–≥–∞–¥–ª–∞–ª ‚Üí “Æ—Ä –¥“Ø–Ω

3Ô∏è‚É£ –•—ç—Ä—ç–≥–∂–∏–ª—Ç–∏–π–Ω –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π
1-—Ä —à–∞—Ç: –¢–æ—Ö–∏—Ä–≥–æ–æ (CONFIG)
–ö–æ–¥:
pythonCONFIG = {
    'sr': 22050,              # Sampling rate
    'duration': 3,            # –ë–∏—á–ª—ç–≥–∏–π–Ω —É—Ä—Ç
    'n_mfcc': 40,            # MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
    'n_fft': 2048,           # FFT —Ü–æ–Ω—Ö
    'hop_length': 512,       # Hop —É—Ä—Ç
    'model_path': 'emotion_model_advanced.pkl',
    'scaler_path': 'scaler.pkl',
    'dataset_path': './audio_dataset/'
}
–¢–∞–π–ª–±–∞—Ä:

sr=22050: –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥—É—É–Ω—ã sampling rate. –°–µ–∫—É–Ω–¥—ç–¥ 22050 —Å—ç–º–ø–ª –∞–≤–Ω–∞. –≠–Ω—ç –Ω—å —Ö“Ø–Ω–∏–π —Ö–æ–æ–ª–æ–π–≥ —Å–∞–π–Ω –∏–ª—Ä“Ø“Ø–ª–Ω—ç.
duration=3: 3 —Å–µ–∫—É–Ω–¥—ã–Ω –¥—É—É –±–∏—á–∏—Ö. –ë–∞–≥–∞–∞—Ä —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π, –∏—Ö ”©–≥”©–≥–¥”©–ª —à–∞–∞—Ä–¥–∞—Ö–≥“Ø–π.
n_mfcc=40: MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∏–π–Ω —Ç–æ–æ. –ò–ª“Ø“Ø –∏—Ö = –∏–ª“Ø“Ø –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª.
n_fft=2048: –î–∞–≤—Ç–∞–º–∂ —à–∏–Ω–∂–ª—ç—Ö—ç–¥ –∞—à–∏–≥–ª–∞—Ö —Ü–æ–Ω—Ö–Ω—ã —Ö—ç–º–∂—ç—ç. 2^11 = —Å—Ç–∞–Ω–¥–∞—Ä—Ç.
hop_length=512: –¶–æ–Ω—Ö–Ω—É—É–¥ —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∑–∞–π. –ë–∞–≥–∞ = –Ω–∞—Ä–∏–π–Ω, –∏—Ö = —Ö—É—Ä–¥–∞–Ω.


2-—Ä —à–∞—Ç: –î—É—É –±–∏—á–∏—Ö (record_audio)
–ö–æ–¥:
pythondef record_audio(duration=None, sr=None):
    if duration is None:
        duration = CONFIG['duration']
    if sr is None:
        sr = CONFIG['sr']
    
    print(f"\nüé§ {duration} —Å–µ–∫—É–Ω–¥—ã–Ω –¥—É—É –±–∏—á–∏–∂ –±–∞–π–Ω–∞...")
    print("   –¢–∞ –æ–¥–æ–æ —è—Ä–∏–Ω–∞ —É—É!")
    
    recording = sd.rec(int(duration * sr), samplerate=sr, 
                      channels=1, dtype='float32')
    sd.wait()
    
    print("‚úÖ –î—É—É –∞–º–∂–∏–ª—Ç—Ç–∞–π –±–∏—á–∏–≥–¥–ª—ç—ç!\n")
    return np.squeeze(recording)
–ê–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä:

–ü–∞—Ä–∞–º–µ—Ç—Ä —à–∞–ª–≥–∞—Ö: –•—ç—Ä—ç–≤ ”©–≥”©”©–≥“Ø–π –±–æ–ª CONFIG-–∞–∞—Å –∞–≤–Ω–∞
–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–¥ –º—ç–¥—ç—ç–ª—ç–ª: –ë–∏—á–∏–∂ —ç—Ö–ª—ç—Ö–∏–π–≥ –º—ç–¥—ç–≥–¥—ç–Ω—ç
sd.rec():

int(duration * sr): –ù–∏–π—Ç —Å—ç–º–ø–ª–∏–π–Ω —Ç–æ–æ (3 —Å–µ–∫ √ó 22050 = 66150 —Å—ç–º–ø–ª)
samplerate=sr: Sampling rate
channels=1: –ú–æ–Ω–æ (—Å—Ç–µ—Ä–µ–æ –±–∏—à)
dtype='float32': ”®–≥”©–≥–¥–ª–∏–π–Ω —Ç”©—Ä”©–ª (-1.0 ~ +1.0)


sd.wait(): –ë–∏—á–ª—ç–≥ –¥—É—É—Å—Ç–∞–ª —Ö“Ø–ª—ç—ç—Ö
np.squeeze(): (66150, 1) ‚Üí (66150,) —Ö—ç–º–∂—ç—ç –±–æ–ª–≥–æ—Ö


3-—Ä —à–∞—Ç: –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö (extract_advanced_features)
–≠–Ω—ç —Ñ—É–Ω–∫—Ü –¥—É—É–Ω–∞–∞—Å 9 —Ç”©—Ä–ª–∏–π–Ω —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–≥ –≥–∞—Ä–≥–∞–Ω–∞.
3.1. –ß–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å—ç–≥ –∞—Ä–∏–ª–≥–∞—Ö
–ö–æ–¥:
pythonaudio, _ = librosa.effects.trim(audio, top_db=25)
–¢–∞–π–ª–±–∞—Ä:

–î—É—É–Ω—ã —ç—Ö—ç–Ω —Ç”©–≥—Å–≥”©–ª–∏–π–Ω —á–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å–≥–∏–π–≥ —Ç–∞—Å–ª–∞—Ö
top_db=25: 25 –¥–µ—Ü–∏–±–µ–ª—ç—ç—Å —á–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å—ç–≥ –≥—ç–∂ “Ø–∑–Ω—ç
“Æ—Ä –¥“Ø–Ω: –ó”©–≤—Ö”©–Ω –¥—É—É –≥–∞—Ä—á –±—É–π —Ö—ç—Å—ç–≥ “Ø–ª–¥—ç–Ω—ç

3.2. MFCC (Mel-Frequency Cepstral Coefficients)
–ö–æ–¥:
python# MFCC
mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=CONFIG['n_mfcc'])

# Delta (1-—Ä –∑—ç—Ä—ç–≥–ª—ç–ª)
mfcc_delta = librosa.feature.delta(mfcc)

# Delta-delta (2-—Ä –∑—ç—Ä—ç–≥–ª—ç–ª)
mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

# –î—É–Ω–¥–∞–∂ –±–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç —Ö–∞–∑–∞–π–ª—Ç
features.extend([
    np.mean(mfcc, axis=1),      # 40 —É—Ç–≥–∞
    np.std(mfcc, axis=1),       # 40 —É—Ç–≥–∞
    np.mean(mfcc_delta, axis=1),    # 40 —É—Ç–≥–∞
    np.mean(mfcc_delta2, axis=1)    # 40 —É—Ç–≥–∞
])
–¢–∞–π–ª–±–∞—Ä:

MFCC: –î—É—É–Ω—ã "”©–Ω–≥”©", —Ç–µ–º–±—Ä–∏–π–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç. –•“Ø–Ω–∏–π —á–∏—Ö –∞–¥–∏–ª –¥–∞–≤—Ç–∞–º–∂ –º—ç–¥—ç—Ä–Ω—ç.
Delta: MFCC-–∏–π–Ω ”©”©—Ä—á–ª”©–ª—Ç–∏–π–Ω —Ö—É—Ä–¥ (—Ö—É–≤–∏—Ä–∞–ª—Ç)
Delta-delta: ”®”©—Ä—á–ª”©–ª—Ç–∏–π–Ω —Ö—É—Ä–¥–∞—Ç–≥–∞–ª (—á–∏–≥ —Ö–∞–Ω–¥–ª–∞–≥–∞)
–Ø–∞–≥–∞–∞–¥ mean/std?: –¶–∞–≥ —Ö—É–≥–∞—Ü–∞–∞–Ω—ã —Ö—É–≤—å—Å–∞–ª—Ç “Ø–ª —Ö–∞–º–∞–∞—Ä–∞—Ö –¥—É–Ω–¥–∞–∂ —É—Ç–≥–∞ –≥–∞—Ä–Ω–∞
–ù–∏–π—Ç features: 40√ó4 = 160 features

3.3. Chroma
–ö–æ–¥:
pythonchroma = librosa.feature.chroma_stft(y=audio, sr=sr)
features.extend([
    np.mean(chroma, axis=1),  # 12 —É—Ç–≥–∞
    np.std(chroma, axis=1)    # 12 —É—Ç–≥–∞
])
–¢–∞–π–ª–±–∞—Ä:

12 —Ö–∞–≥–∞—Å –∞—è—ã–≥ –∏–ª—Ä“Ø“Ø–ª–Ω—ç (C, C#, D, D#, E, F, F#, G, G#, A, A#, B)
–ê—è–ª–≥—É—É, –Ω–æ—Ç–Ω—ã –º—ç–¥—ç—ç–ª—ç–ª
–ù–∏–π—Ç: 12√ó2 = 24 features

3.4. Mel Spectrogram
–ö–æ–¥:
pythonmel = librosa.feature.melspectrogram(y=audio, sr=sr)
features.extend([
    np.mean(mel, axis=1),  # 128 —É—Ç–≥–∞ (default)
    np.std(mel, axis=1)    # 128 —É—Ç–≥–∞
])
–¢–∞–π–ª–±–∞—Ä:

–î–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö—É–≤–∞–∞—Ä—å (—Ö“Ø–Ω–∏–π —á–∏—Ö –∞–¥–∏–ª)
–ë–∞–≥–∞ –¥–∞–≤—Ç–∞–º–∂–∏–¥ –∏–ª“Ø“Ø –º—ç–¥—Ä—ç–≥
–ù–∏–π—Ç: 128√ó2 = 256 features (–æ–π—Ä–æ–ª—Ü–æ–æ–≥–æ–æ—Ä)

3.5. Spectral Contrast
–ö–æ–¥:
pythoncontrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
features.extend([
    np.mean(contrast, axis=1),  # 7 —É—Ç–≥–∞
    np.std(contrast, axis=1)    # 7 —É—Ç–≥–∞
])
–¢–∞–π–ª–±–∞—Ä:

”®–Ω–¥”©—Ä –±–∞ –Ω–∞–º—Ö–∞–Ω –¥–∞–≤—Ç–∞–º–∂–∏–π–Ω –∫–æ–Ω—Ç—Ä–∞—Å—Ç
–î—É—É–Ω—ã "—Ç–µ–∫—Å—Ç—É—Ä"
–ù–∏–π—Ç: 7√ó2 = 14 features

3.6. Tonnetz
–ö–æ–¥:
pythontonnetz = librosa.feature.tonnetz(y=audio, sr=sr)
features.extend([
    np.mean(tonnetz, axis=1),  # 6 —É—Ç–≥–∞
    np.std(tonnetz, axis=1)    # 6 —É—Ç–≥–∞
])
–¢–∞–π–ª–±–∞—Ä:

–ì–∞—Ä–º–æ–Ω–∏–∫ –±“Ø—Ç—ç—Ü
–ê—è–ª–≥—É—É–Ω—É—É–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Ö–∞—Ä–∏–ª—Ü–∞–∞
–ù–∏–π—Ç: 6√ó2 = 12 features

3.7. Zero Crossing Rate (ZCR)
–ö–æ–¥:
pythonzcr = librosa.feature.zero_crossing_rate(audio)
features.append([np.mean(zcr), np.std(zcr)])
–¢–∞–π–ª–±–∞—Ä:

–î–æ—Ö–∏–æ —Ç—ç–≥–∏–π–≥ —Ö—ç–¥—ç–Ω —É–¥–∞–∞ –æ–≥—Ç–ª–æ—Ö
”®–Ω–¥”©—Ä ZCR = —à–∏—Ä“Ø“Ø–Ω –¥—É—É (—É—É—Ä—Ç–∞–π)
–ù–∞–º—Ö–∞–Ω ZCR = –∑”©”©–ª”©–Ω –¥—É—É (–≥—É–Ω–∏–≥—Ç–∞–π)
–ù–∏–π—Ç: 2 features

3.8. Spectral Centroid
–ö–æ–¥:
pythonspectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)
features.append([np.mean(spectral_centroid), np.std(spectral_centroid)])
–¢–∞–π–ª–±–∞—Ä:

–î–∞–≤—Ç–∞–º–∂–∏–π–Ω "—Ç”©–≤"
”®–Ω–¥”©—Ä —É—Ç–≥–∞ = —Ç–æ–¥, –≥—ç—Ä—ç–ª –¥—É—É
–ù–∞–º—Ö–∞–Ω —É—Ç–≥–∞ = –±–∞—Ä–∞–∞–Ω –¥—É—É
–ù–∏–π—Ç: 2 features

3.9. Spectral Rolloff
–ö–æ–¥:
pythonspectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
features.append([np.mean(spectral_rolloff), np.std(spectral_rolloff)])
–¢–∞–π–ª–±–∞—Ä:

–≠—Ä—á–∏–º —Ö“Ø—á–Ω–∏–π 85% –±–∞–≥—Ç–∞–∞—Ö –¥–∞–≤—Ç–∞–º–∂
–î—É—É–Ω—ã "—Ç–æ–¥ –±–∞–π–¥–∞–ª"
–ù–∏–π—Ç: 2 features

3.10. RMS Energy
–ö–æ–¥:
pythonrms = librosa.feature.rms(y=audio)
features.append([np.mean(rms), np.std(rms)])
–¢–∞–π–ª–±–∞—Ä:

–î—É—É–Ω—ã —ç—Ä—á–∏–º —Ö“Ø—á
–ß–∞–Ω–≥–∞/—á–∏–º—ç—ç–≥“Ø–π –±–∞–π–¥–∞–ª
–ù–∏–π—Ç: 2 features

3.11. –ë“Ø—Ö —à–∏–Ω–∂ —á–∞–Ω–∞—Ä –Ω—ç–≥—Ç–≥—ç—Ö
–ö–æ–¥:
pythonfeature_vector = np.concatenate([np.array(f).flatten() for f in features])
return feature_vector.reshape(1, -1)
```

**–¢–∞–π–ª–±–∞—Ä:**
- –ë“Ø—Ö features-–∏–π–≥ –Ω—ç–≥ —É—Ä—Ç –≤–µ–∫—Ç–æ—Ä –±–æ–ª–≥–æ—Ö
- `reshape(1, -1)`: sklearn-–¥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π (1, n_features) —Ö—ç–º–∂—ç—ç
- **–ù–∏–π—Ç features**: ~200-250 –æ—Ä—á–∏–º

---

### 4-—Ä —à–∞—Ç: ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –±—ç–ª—Ç–≥—ç—Ö (`prepare_dataset`)

**”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥–∏–π–Ω –±“Ø—Ç—ç—Ü:**
```
audio_dataset/
‚îú‚îÄ‚îÄ happy/
‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav
‚îÇ   ‚îú‚îÄ‚îÄ audio2.wav
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ sad/
‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ angry/
‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ neutral/
    ‚îú‚îÄ‚îÄ audio1.wav
    ‚îî‚îÄ‚îÄ ...
–ö–æ–¥:
pythondef prepare_dataset(dataset_path=None):
    if dataset_path is None:
        dataset_path = CONFIG['dataset_path']
    
    dataset_path = Path(dataset_path)
    
    if not dataset_path.exists():
        print(f"‚ö†Ô∏è  ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –æ–ª–¥—Å–æ–Ω–≥“Ø–π")
        return create_synthetic_dataset()
    
    X = []  # –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥
    y = []  # Label
    emotions = []
    
    for emotion_folder in dataset_path.iterdir():
        if not emotion_folder.is_dir():
            continue
        
        emotion = emotion_folder.name
        emotions.append(emotion)
        audio_files = list(emotion_folder.glob('*.wav'))
        
        for audio_file in audio_files:
            try:
                audio, sr = librosa.load(audio_file, sr=CONFIG['sr'])
                features = extract_advanced_features(audio, sr)
                
                if features is not None:
                    X.append(features.flatten())
                    y.append(emotion)
            except Exception as e:
                print(f"   ‚ö†Ô∏è {audio_file.name} - –ê–ª–¥–∞–∞: {e}")
    
    return np.array(X), np.array(y), sorted(set(emotions))
–ê–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä:

–ó–∞–º —à–∞–ª–≥–∞—Ö: ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞—Ö
Folder –¥–∞–≤—Ç–∞—Ö: Folder –±“Ø—Ä = 1 —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª
–§–∞–π–ª —É–Ω—à–∏x: .wav —Ñ–∞–π–ª –±“Ø—Ä–∏–π–≥ librosa.load() –∞—à–∏–≥–ª–∞–Ω –∞—á–∞–∞–ª–∞—Ö
Features –≥–∞—Ä–≥–∞—Ö: extract_advanced_features() –¥—É—É–¥–∞—Ö
X, y —Ü—É–≥–ª—É—É–ª–∞—Ö:

X = features (n_samples, ~200)
y = labels (n_samples,)


–ë—É—Ü–∞–∞—Ö: numpy array —Ö—ç–ª–±—ç—Ä—ç—ç—Ä


5-—Ä —à–∞—Ç: –ó–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞—Ö (train_model)
–ö–æ–¥:
pythondef train_model(X, y, emotions):
    print("üå≤ –ó–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
    
    # 80/20 —Ö—É–≤–∞–∞–ª—Ç
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–ª
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # RandomForest
    model = RandomForestClassifier(
        n_estimators=300,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )
    
    model.fit(X_train_scaled, y_train)
    
    # “Æ–Ω—ç–ª–≥—ç—ç
    y_pred = model.predict(X_test_scaled)
    accuracy = model.score(X_test_scaled, y_test)
    
    print(f"\nüìà –ù–∞—Ä–∏–π–≤—á–ª–∞–ª: {accuracy * 100:.2f}%\n")
    print(classification_report(y_test, y_pred, target_names=emotions))
    
    # Confusion matrix
    plot_confusion_matrix(y_test, y_pred, emotions)
    
    # –•–∞–¥–≥–∞–ª–∞—Ö
    joblib.dump(model, CONFIG['model_path'])
    joblib.dump(scaler, CONFIG['scaler_path'])
    
    return model, scaler
–ê–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä:
5.1. ”®–≥”©–≥–¥”©–ª —Ö—É–≤–∞–∞—Ö
pythonX_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

80% —Å—É—Ä–≥–∞–ª—Ç, 20% —Ç–µ—Å—Ç
stratify=y: Label –±“Ø—Ä –ø—Ä–æ–ø–æ—Ä—Ü —Ö–∞–¥–≥–∞–ª–Ω–∞ (–∂–∏—à—ç—ç: happy –¥—É—É–Ω—ã 80% —Å—É—Ä–≥–∞–ª—Ç–∞–Ω–¥, 20% —Ç–µ—Å—Ç—ç–¥)
random_state=42: –î–∞–≤—Ç–∞–≥–¥–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π “Ø—Ä –¥“Ø–Ω

5.2. –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–ª
pythonscaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
–Ø–∞–≥–∞–∞–¥ —Ö—ç—Ä—ç–≥—Ç—ç–π?

Features-—É—É–¥ ”©”©—Ä ”©”©—Ä scale-—Ç—ç–π (MFCC: -100100, RMS: 01)
–°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª—Å–Ω—ã –¥–∞—Ä–∞–∞: –¥—É–Ω–¥–∞–∂=0, —Å—Ç–∞–Ω–¥–∞—Ä—Ç —Ö–∞–∑–∞–π–ª—Ç=1
–ó–∞–≥–≤–∞—Ä —Ö—É—Ä–¥–∞–Ω —Å—É—Ä–∞–ª—Ü–∞–Ω–∞

–ê–Ω—Ö–∞–∞—Ä–∞—Ö!

fit_transform(): –°—É—Ä–≥–∞–ª—Ç –¥—ç—ç—Ä —Å—É—Ä–∞–ª—Ü–∞—Ö
transform(): –¢–µ—Å—Ç –¥—ç—ç—Ä –∑”©–≤—Ö”©–Ω —Ö—ç—Ä—ç–≥–ª—ç—Ö (overfit-—ç—ç—Å —Å—ç—Ä–≥–∏–π–ª–Ω—ç)

5.3. RandomForest –∑–∞–≥–≤–∞—Ä
pythonmodel = RandomForestClassifier(
    n_estimators=300,      # 300 –º–æ–¥
    max_depth=20,          # –ú–æ–¥–Ω—ã –≥“Ø–Ω
    min_samples_split=5,   # Node —Ö—É–≤–∞–∞—Ö–∞–¥ 5+ —Å—ç–º–ø–ª
    min_samples_leaf=2,    # –ù–∞–≤—á node-–¥ 2+ —Å—ç–º–ø–ª
    random_state=42,
    n_jobs=-1,             # –ë“Ø—Ö CPU
    class_weight='balanced'  # Label —Ç—ç–Ω—Ü“Ø“Ø
)
–ü–∞—Ä–∞–º–µ—Ç—Ä“Ø“Ø–¥–∏–π–Ω —Ç–∞–π–ª–±–∞—Ä:
–ü–∞—Ä–∞–º–µ—Ç—Ä–£—Ç–≥–∞–¢–∞–π–ª–±–∞—Än_estimators300300 –º–æ–¥ “Ø“Ø—Å–≥—ç–Ω—ç. –ò—Ö = –Ω–∞–π–¥–≤–∞—Ä—Ç–∞–π, —É–¥–∞–∞–Ωmax_depth20–ú–æ–¥–Ω—ã —Ö–∞–º–≥–∏–π–Ω –∏—Ö –≥“Ø–Ω. Overfit-—ç—ç—Å —Å—ç—Ä–≥–∏–π–ª–Ω—çmin_samples_split5Node —Ö—É–≤–∞–∞—Ö–∞–¥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π —Ö–∞–º–≥–∏–π–Ω –±–∞–≥–∞ —Å—ç–º–ø–ªmin_samples_leaf2–ù–∞–≤—á node-–¥ –±–∞–π—Ö —Ö–∞–º–≥–∏–π–Ω –±–∞–≥–∞ —Å—ç–º–ø–ªrandom_state42–î–∞–≤—Ç–∞–≥–¥–∞—Ö “Ø—Ä –¥“Ø–Ωn_jobs-1–ë“Ø—Ö CPU core –∞—à–∏–≥–ª–∞—Öclass_weightbalanced–¶”©”©–Ω label-–¥ –∏–ª“Ø“Ø –∂–∏–Ω ”©–≥”©—Ö
5.4. –°—É—Ä–≥–∞—Ö
pythonmodel.fit(X_train_scaled, y_train)

300 –º–æ–¥ “Ø“Ø—Å–≥—ç–Ω—ç
–ú–æ–¥ –±“Ø—Ä ”©”©—Ä ”©”©—Ä —Å—ç–º–ø–ª“Ø“Ø–¥ –¥—ç—ç—Ä —Å—É—Ä–∞–ª—Ü–∞–Ω–∞
–î—É–Ω–¥–∂–∞–∞—Ä —Å–∞–Ω–∞–ª ”©–≥–Ω”© (voting)

5.5. “Æ–Ω—ç–ª–≥—ç—ç
pythony_pred = model.predict(X_test_scaled)
accuracy = model.score(X_test_scaled, y_test)
print(classification_report(y_test, y_pred))
```

**Classification Report:**
```
              precision    recall  f1-score   support

       angry       0.85      0.83      0.84        30
       happy       0.88      0.90      0.89        40
     neutral       0.82      0.80      0.81        35
         sad       0.87      0.89      0.88        38

    accuracy                           0.86       143
–¢–∞–π–ª–±–∞—Ä:

Precision: –¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω "happy"-–Ω–∏–π —Ö—ç–¥—ç–Ω —Ö—É–≤—å –Ω—å “Ø–Ω—ç—Ö—ç—ç—Ä "happy" –≤—ç?
Recall: –ë“Ø—Ö "happy" –¥—É—É–Ω—ã —Ö—ç–¥—ç–Ω —Ö—É–≤–∏–π–≥ –Ω—å –æ–ª—Å–æ–Ω –±—ç?
F1-score: Precision –±–∞ Recall-–∏–π–Ω –¥—É–Ω–¥–∞–∂
Support: –¢–µ—Å—Ç –¥—ç—ç—Ä—Ö —Ç—É—Ö–∞–π–Ω label-—ã–Ω —Ç–æ–æ

5.6. Confusion Matrix
pythonplot_confusion_matrix(y_test, y_pred, emotions)
```

**–ñ–∏—à—ç—ç “Ø—Ä –¥“Ø–Ω:**
```
           happy  sad  angry  neutral
happy        36    2     1      1
sad           1   34     2      1
angry         0    1    25      4
neutral       2    1     2     30
–¢–∞–π–ª–±–∞—Ä:

–î–∏–∞–≥–æ–Ω–∞–ª—å: –ó”©–≤ —Ç–∞–∞–º–∞–≥
–ë—É—Å–∞–¥: –ê–ª–¥–∞–∞ (–∂–∏—à—ç—ç: happy –≥—ç–∂ –±–æ–¥—Å–æ–Ω –±–æ–ª–æ–≤—á sad –±–∞–π—Å–∞–Ω)

5.7. –•–∞–¥–≥–∞–ª–∞—Ö
pythonjoblib.dump(model, CONFIG['model_path'])
joblib.dump(scaler, CONFIG['scaler_path'])

–ó–∞–≥–≤–∞—Ä—ã–≥ .pkl —Ñ–∞–π–ª –±–æ–ª–≥–æ—Ö
–î–∞—Ä–∞–∞ –¥–∞—Ö–∏–Ω —Å—É—Ä–≥–∞—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞–≥“Ø–π
–•—É—Ä–¥–∞–Ω –∞—á–∞–∞–ª–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π


6-—Ä —à–∞—Ç: –¢–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö (predict_emotion)
–ö–æ–¥:
pythondef predict_emotion(model, scaler, features):
    if features is None:
        return None, None, None
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞—Ö
    features_scaled = scaler.transform(features)
    
    # –ú–∞–≥–∞–¥–ª–∞–ª
    probabilities = model.predict_proba(features_scaled)[0]
    
    # Label-—É—É–¥
    emotions = model.classes_
    
    # –•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä –º–∞–≥–∞–¥–ª–∞–ª
    best_emotion = emotions[np.argmax(probabilities)]
    
    return emotions, probabilities, best_emotion
–ê–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä:

–°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞—Ö: –°—É—Ä–≥–∞–ª—Ç “Ø–µ–¥ —Ö–∏–π—Å—ç–Ω –∞–¥–∏–ª
predict_proba(): Label –±“Ø—Ä–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª—ã–≥ —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö

python   # –ñ–∏—à—ç—ç “Ø—Ä –¥“Ø–Ω:
   probabilities = [0.15, 0.62, 0.08, 0.10, 0.05]
   #                happy  sad   angry neutral fear

–•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä —Å–æ–Ω–≥–æ—Ö: np.argmax() –∞—à–∏–≥–ª–∞—Ö
–ë—É—Ü–∞–∞—Ö: emotions, probabilities, best_emotion


7-—Ä —à–∞—Ç: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏ (plot_emotion_results)
–ö–æ–¥:
pythondef plot_emotion_results(emotions, probabilities, audio, sr):
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. –ú–∞–≥–∞–¥–ª–∞–ª—ã–Ω –≥—Ä–∞—Ñ–∏–∫
    ax1 = axes[0, 0]
    colors = plt.cm.viridis(probabilities / probabilities.max())
    bars = ax1.bar(emotions, probabilities, color=colors)
    # ... (–∫–æ–¥ “Ø—Ä–≥—ç–ª–∂–ª—ç—Ö)
    
    # 2. Waveform
    ax2 = axes[0, 1]
    time = np.arange(len(audio)) / sr
    ax2.plot(time, audio, color='steelblue')
    # ...
    
    # 3. Mel Spectrogram
    ax3 = axes[1, 0]
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max())
    librosa.display.specshow(mel_spec_db, sr=sr, ax=ax3)
    # ...
    
    # 4. MFCC
    ax4 = axes[1, 1]
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
    librosa.display.specshow(mfcc, sr=sr, ax=ax4)
    # ...
    
    plt.tight_layout()
    plt.savefig('emotion_analysis.png', dpi=150)
    plt.show()
4 –≥—Ä–∞—Ñ–∏–∫:
7.1. –ú–∞–≥–∞–¥–ª–∞–ª—ã–Ω Bar Chart

X —Ç—ç–Ω—Ö–ª—ç–≥: –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥
Y —Ç—ç–Ω—Ö–ª—ç–≥: –ú–∞–≥–∞–¥–ª–∞–ª (0-1)
–•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä: –£–ª–∞–∞–Ω —Ö“Ø—Ä—ç—ç
”®–Ω–≥”©: Mag–∞–¥–ª–∞–ª –∏—Ö—Å—ç—Ö —Ç—É—Å–∞–º –≥—ç—Ä—ç–ª

7.2. Waveform (–î—É—É —Ö–æ–æ–ª–æ–π–Ω –¥–æ–ª–≥–∏–æ–Ω)

X —Ç—ç–Ω—Ö–ª—ç–≥: –¶–∞–≥ —Ö—É–≥–∞—Ü–∞–∞ (—Å–µ–∫—É–Ω–¥)
Y —Ç—ç–Ω—Ö–ª—ç–≥: –ê–º–ø–ª–∏—Ç—É–¥ (-1 ~ +1)
–î—É—É–Ω—ã —Ö—ç–≤ –≥–∞–∂–∏–ª—Ç —Ö–∞—Ä–∞–≥–¥–∞–Ω–∞

7.3. Mel Spectrogram

X —Ç—ç–Ω—Ö–ª—ç–≥: –¶–∞–≥ —Ö—É–≥–∞—Ü–∞–∞
Y —Ç—ç–Ω—Ö–ª—ç–≥: –î–∞–≤—Ç–∞–º–∂ (Hz)
”®–Ω–≥”©: –≠—Ä—á–∏–º —Ö“Ø—á (—Ü—ç–Ω—Ö—ç—Ä=–±–∞–≥–∞, —É–ª–∞–∞–Ω=–∏—Ö)

7.4. MFCC
X —Ç—ç–Ω—Ö–ª—ç–≥: –¶–∞–≥ —Ö—É–≥–∞—Ü–∞–∞
Y —Ç—ç–Ω—Ö–ª—ç–≥: MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (1-20)
”®–Ω–≥”©: MFCC —É—Ç–≥–∞ (—Ü—ç–Ω—Ö—ç—Ä=–±–∞–≥–∞, —É–ª–∞–∞–Ω=–∏—Ö)

---
### 8Ô∏è‚É£ –î“Ø–≥–Ω—ç–ª—Ç
–≠–Ω—ç—Ö“Ø“Ø —Ç”©—Å–ª–∏–π–Ω —Ö“Ø—Ä—ç—ç–Ω–¥ –¥—É—É —Ö–æ–æ–ª–æ–π–Ω —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª
—Ç–∞–Ω–∏—Ö —Å–∏—Å—Ç–µ–º–∏–π–≥ –∞–º–∂–∏–ª—Ç—Ç–∞–π –±“Ø—Ç—ç—ç–∂, RandomForest –∑–∞–≥–≤–∞—Ä—ã–≥ –∞—à–∏–≥–ª–∞–Ω 80%+ –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª—Ç–∞–π —Ç–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π –±–æ–ª–ª–æ–æ. –¶–∞–∞—à–∏–¥ ”©–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥ ”©—Ä–≥”©–∂“Ø“Ø–ª–∂, –≥“Ø–Ω–∑–≥–∏–π —Å—É—Ä–∞–ª—Ü–∞—Ö (Deep Learning) –∞—Ä–≥—É—É–¥—ã–≥ —Ç—É—Ä—à–∏–∂ “Ø–∑—ç—Ö—ç—ç—Ä —Ç”©–ª”©–≤–ª”©–∂ –±–∞–π–Ω–∞.


