import sounddevice as sd  # –ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä –¥—É—É –±–∏—á–∏—Ö—ç–¥ –∞—à–∏–≥–ª–∞–Ω–∞
import librosa  # –ê—É–¥–∏–æ —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö, —à–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–Ω–∞
import numpy as np  # –ú–∞—Ç–µ–º–∞—Ç–∏–∫ —Ç–æ–æ—Ü–æ–æ–ª–æ–ª, –º–∞—Å—Å–∏–≤ –∞–∂–∏–ª–ª–∞–≥–∞–∞–Ω–¥ –∞—à–∏–≥–ª–∞–Ω–∞
import matplotlib.pyplot as plt  # –ì—Ä–∞—Ñ–∏–∫ –∑—É—Ä–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–Ω–∞
import joblib  # –ó–∞–≥–≤–∞—Ä—ã–≥ —Ñ–∞–π–ª –±–æ–ª–≥–æ–Ω —Ö–∞–¥–≥–∞–ª–∞—Ö, –∞—á–∞–∞–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–Ω–∞
import os  # –§–∞–π–ª—ã–Ω —Å–∏—Å—Ç–µ–º –¥—ç—ç—Ä –∞–∂–∏–ª–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–Ω–∞
from sklearn.ensemble import RandomForestClassifier  # –ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º
from sklearn.model_selection import train_test_split  # ”®–≥”©–≥–¥–ª–∏–π–≥ —Å—É—Ä–≥–∞–ª—Ç/—Ç–µ—Å—Ç –≥—ç–∂ —Ö—É–≤–∞–∞—Ö–∞–¥
from sklearn.metrics import classification_report, confusion_matrix  # –ó–∞–≥–≤–∞—Ä—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ “Ø–Ω—ç–ª—ç—Ö—ç–¥
import seaborn as sns  # –ì—Ä–∞—Ñ–∏–∫ –≥–æ—ë —Ö–∞—Ä—É—É–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–Ω–∞
from pathlib import Path  # –§–∞–π–ª—ã–Ω –∑–∞–º –∑–æ—Ö–∏—Ü—É—É–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä –∞—Ä–≥–∞
import warnings  # –ê–Ω—Ö–∞–∞—Ä—É—É–ª–≥–∞ –º–µ—Å—Å–µ–∂“Ø“Ø–¥–∏–π–≥ —É–¥–∏—Ä–¥–∞—Ö–∞–¥
warnings.filterwarnings('ignore')  # –ë“Ø—Ö –∞–Ω—Ö–∞–∞—Ä—É—É–ª–≥–∞ –º–µ—Å—Å–µ–∂–∏–π–≥ –Ω—É—É—Ö

# =====================================================
#  –¢–û–•–ò–†–ì–û–û - –°–∏—Å—Ç–µ–º–∏–π–Ω “Ø–Ω–¥—Å—ç–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä“Ø“Ø–¥
# =====================================================
CONFIG = {
    'sr': 22050,              # Sampling rate - –°–µ–∫—É–Ω–¥—ç–¥ —Ö—ç–¥—ç–Ω —Å—ç–º–ø–ª –∞–≤–∞—Ö (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥—É—É)
    'duration': 3,            # –ë–∏—á–ª—ç–≥–∏–π–Ω —É—Ä—Ç (—Å–µ–∫—É–Ω–¥) - –•—ç–¥—ç–Ω —Å–µ–∫—É–Ω–¥ –¥—É—É –±–∏—á–∏—Ö
    'n_mfcc': 40,            # MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç“Ø“Ø–¥ - –î—É—É–Ω—ã –æ–Ω—Ü–ª–æ–≥ —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–Ω —Ç–æ–æ
    'n_fft': 2048,           # FFT —Ü–æ–Ω—Ö–Ω—ã —Ö—ç–º–∂—ç—ç - –î–∞–≤—Ç–∞–º–∂ —à–∏–Ω–∂–ª—ç—Ö—ç–¥ –∞—à–∏–≥–ª–∞—Ö
    'hop_length': 512,       # Hop —É—Ä—Ç - –¶–æ–Ω—Ö–Ω—É—É–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∑–∞–π
    'model_path': 'emotion_model_advanced.pkl',  # –ó–∞–≥–≤–∞—Ä—ã–≥ —Ö–∞–∞–Ω–∞ —Ö–∞–¥–≥–∞–ª–∞—Ö
    'scaler_path': 'scaler.pkl',  # ”®–≥”©–≥–¥”©–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞—Ö –∑–∞–≥–≤–∞—Ä—ã–≥ —Ö–∞–∞–Ω–∞ —Ö–∞–¥–≥–∞–ª–∞—Ö
    'dataset_path': './audio_dataset/'  # ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥ —Ö–∞–∞–Ω–∞–∞—Å —É–Ω—à–∏–∂ –∞–≤–∞—Ö
}

# =====================================================
#  –î–£–£ –ë–ò–ß–ò–• - –ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä –¥—É—É –±–∏—á–∏—Ö —Ñ—É–Ω–∫—Ü
# =====================================================
def record_audio(duration=None, sr=None):
    """
    –ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä –¥—É—É –±–∏—á–∏—Ö —Ñ—É–Ω–∫—Ü
    
    Parameters:
        duration: –ë–∏—á–ª—ç–≥–∏–π–Ω —É—Ä—Ç (—Å–µ–∫—É–Ω–¥). None –±–æ–ª CONFIG-–∞–∞—Å –∞–≤–Ω–∞
        sr: Sampling rate. None –±–æ–ª CONFIG-–∞–∞—Å –∞–≤–Ω–∞
        
    Returns:
        recording: Numpy array —Ö—ç–ª–±—ç—Ä–∏–π–Ω –¥—É—É –±–∏—á–ª—ç–≥
    """
    # –•—ç—Ä—ç–≤ —É—Ç–≥–∞ ”©–≥”©”©–≥“Ø–π –±–æ–ª CONFIG-–∞–∞—Å –∞–≤–∞—Ö
    if duration is None:
        duration = CONFIG['duration']
    if sr is None:
        sr = CONFIG['sr']
    
    # –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–¥ –º—ç–¥—ç—ç–ª—ç–ª ”©–≥”©—Ö
    print(f"\nüé§ {duration} —Å–µ–∫—É–Ω–¥—ã–Ω –¥—É—É –±–∏—á–∏–∂ –±–∞–π–Ω–∞...")
    print("   –¢–∞ –æ–¥–æ–æ —è—Ä–∏–Ω–∞ —É—É!")
    
    # –ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä –±–∏—á–∏—Ö: duration*sr = –Ω–∏–π—Ç —Å—ç–º–ø–ª–∏–π–Ω —Ç–æ–æ
    # samplerate=sr: —Å–µ–∫—É–Ω–¥—ç–¥ —Ö—ç–¥—ç–Ω —Å—ç–º–ø–ª –∞–≤–∞—Ö
    # channels=1: –º–æ–Ω–æ –¥—É—É (1 —Å—É–≤–∞–≥)
    # dtype='float32': ”©–≥”©–≥–¥–ª–∏–π–Ω —Ç”©—Ä”©–ª
    recording = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')
    sd.wait()  # –ë–∏—á–ª—ç–≥ –¥—É—É—Å—Ç–∞–ª —Ö“Ø–ª—ç—ç—Ö
    
    print("‚úÖ –î—É—É –∞–º–∂–∏–ª—Ç—Ç–∞–π –±–∏—á–∏–≥–¥–ª—ç—ç!\n")
    return np.squeeze(recording)  # 2D –º–∞—Å—Å–∏–≤—ã–≥ 1D –±–æ–ª–≥–æ—Ö

# =====================================================
#  –î–≠–í–®–ò–õ–¢–≠–¢ –®–ò–ù–ñ –ß–ê–ù–ê–† –ì–ê–†–ì–ê–•
# =====================================================
def extract_advanced_features(audio, sr=None):
    """
    –û–ª–æ–Ω —Ç”©—Ä–ª–∏–π–Ω —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–≥ –≥–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö —Ñ—É–Ω–∫—Ü
    
    –ì–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥:
    1. MFCC - –î—É—É–Ω—ã ”©–Ω–≥”©, —Ç–µ–º–±—Ä–∏–π–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç
    2. Chroma - –ê—è–ª–≥—É—É, —Ö”©–≥–∂–º–∏–π–Ω –Ω–æ—Ç –º—ç–¥—Ä—ç—Ö
    3. Mel Spectrogram - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö—É–≤–∞–∞—Ä—å
    4. Spectral Contrast - ”®–Ω–¥”©—Ä –±–∞ –Ω–∞–º—Ö–∞–Ω –¥–∞–≤—Ç–∞–º–∂–∏–π–Ω —è–ª–≥–∞–∞
    5. Tonnetz - –ì–∞—Ä–º–æ–Ω–∏–∫ –±“Ø—Ç—ç—Ü, –∞—è–ª–≥—É—É —Ö–æ–ª–±–æ–æ
    6. Zero Crossing Rate - –î–æ—Ö–∏–æ —Ç—ç–≥ –æ–≥—Ç–ª–æ—Ö —Ö—É—Ä–¥ (–¥—É—É–Ω—ã —à–∏—Ä“Ø“Ø–Ω –±–∞–π–¥–∞–ª)
    7. Spectral Centroid - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ç”©–≤ (–¥—É—É–Ω—ã "—Ç–æ–¥ –±–∞–π–¥–∞–ª")
    8. Spectral Rolloff - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö—è–∑–≥–∞–∞—Ä
    9. RMS Energy - –î—É—É —á–∏–º—ç—ç–Ω–∏–π —ç—Ä—á–∏–º —Ö“Ø—á
    
    Parameters:
        audio: –î—É—É —Ö–æ–æ–ª–æ–π–Ω numpy array
        sr: Sampling rate
        
    Returns:
        feature_vector: –ë“Ø—Ö —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–≥ –∞–≥—É—É–ª—Å–∞–Ω –≤–µ–∫—Ç–æ—Ä (1, n) —Ö—ç–º–∂—ç—ç—Ç—ç–π
    """
    if sr is None:
        sr = CONFIG['sr']
    
    # –ß–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å–≥–∏–π–≥ –∞—Ä–∏–ª–≥–∞—Ö (–¥—É—É–Ω—ã —ç—Ö—ç–Ω —Ç”©–≥—Å–≥”©–ª–∏–π–Ω —á–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å—ç–≥)
    # top_db=25: 25 –¥–µ—Ü–∏–±–µ–ª—ç—ç—Å —á–∏–º—ç—ç–≥“Ø–π —Ö—ç—Å–≥–∏–π–≥ –∞—Ä–∏–ª–≥–∞—Ö
    audio, _ = librosa.effects.trim(audio, top_db=25)
    
    features = []  # –ë“Ø—Ö —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–≥ —ç–Ω–¥ —Ü—É–≥–ª—É—É–ª–Ω–∞
    
    try:
        # ===== 1. MFCC –±–æ–ª–æ–Ω —Ç“Ø“Ø–Ω–∏–π —Ç—É—É—Ä–≤–∞–ª—É—É–¥ =====
        # MFCC: –î—É—É–Ω—ã –æ–Ω—Ü–ª–æ–≥ —Ç–µ–º–±—Ä, ”©–Ω–≥–∏–π–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=CONFIG['n_mfcc'])
        
        # Delta (1-—Ä –∑—ç—Ä—ç–≥–ª—ç–ª–∏–π–Ω —Ç—É—É—Ä–≤–∞–ª): MFCC-–∏–π–Ω ”©”©—Ä—á–ª”©–ª—Ç–∏–π–Ω —Ö—É—Ä–¥
        mfcc_delta = librosa.feature.delta(mfcc)
        
        # Delta-delta (2-—Ä –∑—ç—Ä—ç–≥–ª—ç–ª–∏–π–Ω —Ç—É—É—Ä–≤–∞–ª): ”®”©—Ä—á–ª”©–ª—Ç–∏–π–Ω “Ø—Ä–≥—ç–ª–∂–ª—ç—Ö —á–∏–≥ —Ö–∞–Ω–¥–ª–∞–≥–∞
        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)
        
        # –î—É–Ω–¥–∞–∂ –±–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç —Ö–∞–∑–∞–π–ª—Ç—ã–≥ –∞–≤–∞—Ö (—Ü–∞–≥ —Ö—É–≥–∞—Ü–∞–∞–Ω—ã –¥—É–Ω–¥–∞–∂)
        features.extend([
            np.mean(mfcc, axis=1),      # MFCC-–∏–π–Ω –¥—É–Ω–¥–∞–∂
            np.std(mfcc, axis=1),       # MFCC-–∏–π–Ω —Ö–∞–∑–∞–π–ª—Ç
            np.mean(mfcc_delta, axis=1),    # ”®”©—Ä—á–ª”©–ª—Ç–∏–π–Ω –¥—É–Ω–¥–∞–∂
            np.mean(mfcc_delta2, axis=1)    # –•—É—Ä–¥–∞—Ç–≥–∞–ª—ã–Ω –¥—É–Ω–¥–∞–∂
        ])
        
        # ===== 2. Chroma - –ê—è–ª–≥—É—É, –Ω–æ—Ç–Ω—ã –º—ç–¥—ç—ç–ª—ç–ª =====
        # 12 —Ö–∞–≥–∞—Å –∞—è—ã–≥ (C, C#, D, ... B) –∏–ª—Ä“Ø“Ø–ª–Ω—ç
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
        features.extend([
            np.mean(chroma, axis=1),  # –ê—è–ª–≥—É—É–Ω—ã –¥—É–Ω–¥–∞–∂
            np.std(chroma, axis=1)    # –ê—è–ª–≥—É—É–Ω—ã —Ö–∞–∑–∞–π–ª—Ç
        ])
        
        # ===== 3. Mel Spectrogram - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω –∑–∞–¥–∞—Ä–≥–∞–∞ =====
        # –•“Ø–Ω–∏–π —á–∏—Ö –∞–¥–∏–ª –¥–∞–≤—Ç–∞–º–∂–∏–π–≥ –º—ç–¥—Ä—ç—Ö (–±–∞–≥–∞ –¥–∞–≤—Ç–∞–º–∂–∏–¥ –∏–ª“Ø“Ø –º—ç–¥—Ä—ç–≥)
        mel = librosa.feature.melspectrogram(y=audio, sr=sr)
        features.extend([
            np.mean(mel, axis=1),  # Mel –¥–∞–≤—Ç–∞–º–∂–∏–π–Ω –¥—É–Ω–¥–∞–∂
            np.std(mel, axis=1)    # Mel –¥–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö–∞–∑–∞–π–ª—Ç
        ])
        
        # ===== 4. Spectral Contrast - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω –∫–æ–Ω—Ç—Ä–∞—Å—Ç =====
        # ”®–Ω–¥”©—Ä –±–∞ –Ω–∞–º—Ö–∞–Ω –¥–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —è–ª–≥–∞–∞–≥ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–Ω–æ
        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
        features.extend([
            np.mean(contrast, axis=1),  # –ö–æ–Ω—Ç—Ä–∞—Å—Ç—ã–Ω –¥—É–Ω–¥–∞–∂
            np.std(contrast, axis=1)    # –ö–æ–Ω—Ç—Ä–∞—Å—Ç—ã–Ω —Ö–∞–∑–∞–π–ª—Ç
        ])
        
        # ===== 5. Tonnetz - –ì–∞—Ä–º–æ–Ω–∏–∫ –±“Ø—Ç—ç—Ü =====
        # –ê—è–ª–≥—É—É–Ω—É—É–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Ö–∞—Ä–∏–ª—Ü–∞–∞–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç
        tonnetz = librosa.feature.tonnetz(y=audio, sr=sr)
        features.extend([
            np.mean(tonnetz, axis=1),  # –ì–∞—Ä–º–æ–Ω–∏–∫–∏–π–Ω –¥—É–Ω–¥–∞–∂
            np.std(tonnetz, axis=1)    # –ì–∞—Ä–º–æ–Ω–∏–∫–∏–π–Ω —Ö–∞–∑–∞–π–ª—Ç
        ])
        
        # ===== 6. Zero Crossing Rate - –¢—ç–≥ –æ–≥—Ç–ª–æ—Ö —Ö—É—Ä–¥ =====
        # –î–æ—Ö–∏–æ —Ç—ç–≥–∏–π–≥ —Ö—ç–¥—ç–Ω —É–¥–∞–∞ –æ–≥—Ç–ª–æ—Ö (–¥—É—É–Ω—ã —à–∏—Ä“Ø“Ø–Ω –±–∞–π–¥–ª—ã–≥ –∏–ª—ç—Ä—Ö–∏–π–ª–Ω—ç)
        zcr = librosa.feature.zero_crossing_rate(audio)
        features.append([np.mean(zcr), np.std(zcr)])
        
        # ===== 7. Spectral Centroid - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ç”©–≤ =====
        # –î—É—É–Ω—ã "—Ç–æ–¥ –±–∞–π–¥–∞–ª" - ”®–Ω–¥”©—Ä —É—Ç–≥–∞ = –≥—ç—Ä—ç–ª —Ç–æ–¥ –¥—É—É
        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)
        features.append([np.mean(spectral_centroid), np.std(spectral_centroid)])
        
        # ===== 8. Spectral Rolloff - –î–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö—è–∑–≥–∞–∞—Ä =====
        # –°–ø–µ–∫—Ç—Ä–∏–π–Ω —ç—Ä—á–∏–º —Ö“Ø—á–Ω–∏–π 85% –±–∞–≥—Ç–∞–∞—Ö –¥–æ–æ–¥ –¥–∞–≤—Ç–∞–º–∂
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
        features.append([np.mean(spectral_rolloff), np.std(spectral_rolloff)])
        
        # ===== 9. RMS Energy - –î—É—É —á–∏–º—ç—ç–Ω–∏–π —ç—Ä—á–∏–º =====
        # –î—É—É–Ω—ã —Ö“Ø—á —á–∞–¥–∞–ª, —á–∞–Ω–≥–∞ –±–∞–π–¥–∞–ª
        rms = librosa.feature.rms(y=audio)
        features.append([np.mean(rms), np.std(rms)])
        
        # –ë“Ø—Ö —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—ã–≥ –Ω—ç–≥ —É—Ä—Ç –≤–µ–∫—Ç–æ—Ä –±–æ–ª–≥–æ—Ö
        # flatten(): —Ö—ç–º–∂—ç—ç—Å –Ω—å (n,) –±–æ–ª–≥–æ–Ω–æ
        # concatenate(): –±“Ø—Ö –º–∞—Å—Å–∏–≤—É—É–¥—ã–≥ —Ö–æ–ª–±–æ–Ω–æ
        feature_vector = np.concatenate([np.array(f).flatten() for f in features])
        
        # Reshape —Ö–∏–π–∂ (1, n) —Ö—ç–º–∂—ç—ç—Ç—ç–π –±–æ–ª–≥–æ—Ö (sklearn-–¥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π)
        return feature_vector.reshape(1, -1)
        
    except Exception as e:
        print(f"‚ùå –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}")
        return None

# =====================================================
#  ”®–ì”®–ì–î–õ–ò–ô–ù –°–ê–ù –ë–≠–õ–¢–ì–≠–•
# =====================================================
def prepare_dataset(dataset_path=None):
    """
    ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥ –∞—á–∞–∞–ª–∂, —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥ –≥–∞—Ä–≥–∞—Ö
    
    ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥–∏–π–Ω –±“Ø—Ç—ç—Ü:
    audio_dataset/
    ‚îú‚îÄ‚îÄ happy/           # –ë–∞—è—Ä —Ö”©”©—Ä—Ç—ç–π –¥—É—É
    ‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav
    ‚îÇ   ‚îú‚îÄ‚îÄ audio2.wav
    ‚îú‚îÄ‚îÄ sad/             # –ì—É–Ω–∏–≥—Ç–∞–π –¥—É—É
    ‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav
    ‚îú‚îÄ‚îÄ angry/           # –£—É—Ä—Ç–∞–π –¥—É—É
    ‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav
    ‚îî‚îÄ‚îÄ neutral/         # –¢”©–≤–∏–π–≥ —Å–∞—Ö–∏—Å–∞–Ω –¥—É—É
        ‚îú‚îÄ‚îÄ audio1.wav
        
    Returns:
        X: –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–Ω numpy array (n_samples, n_features)
        y: Label –±—É—é—É —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –Ω—ç—Ä—Å (n_samples,)
        emotions: –ë“Ø—Ö —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –∂–∞–≥—Å–∞–∞–ª—Ç
    """
    if dataset_path is None:
        dataset_path = CONFIG['dataset_path']
    
    dataset_path = Path(dataset_path)  # –ó–∞–º –∑–æ—Ö–∏—Ü—É—É–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä
    
    # –•—ç—Ä—ç–≤ ”©–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –±–∞–π—Ö–≥“Ø–π –±–æ–ª –∂–∏—à—ç—ç “Ø“Ø—Å–≥—ç—Ö
    if not dataset_path.exists():
        print(f"‚ö†Ô∏è  ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –æ–ª–¥—Å–æ–Ω–≥“Ø–π: {dataset_path}")
        print("üìÅ –ñ–∏—à—ç—ç ”©–≥”©–≥–¥”©–ª “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
        return create_synthetic_dataset()
    
    X = []  # –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥
    y = []  # Label (—Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª)
    emotions = []  # –ë“Ø—Ö –±–æ–ª–æ–º–∂–∏—Ç —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥
    
    print("üìä ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∂ –±–∞–π–Ω–∞...")
    
    # Folder —Ç—É—Å –±“Ø—Ä–∏–π–≥ (happy, sad –≥—ç—Ö –º—ç—Ç) –¥–∞–≤—Ç–∞—Ö
    for emotion_folder in dataset_path.iterdir():
        if not emotion_folder.is_dir():  # –ó”©–≤—Ö”©–Ω folder-—É—É–¥—ã–≥
            continue
        
        emotion = emotion_folder.name  # Folder-—ã–Ω –Ω—ç—Ä = —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª
        emotions.append(emotion)
        audio_files = list(emotion_folder.glob('*.wav'))  # .wav —Ñ–∞–π–ª—É—É–¥
        
        print(f"   {emotion}: {len(audio_files)} —Ñ–∞–π–ª")
        
        # –ê—É–¥–∏–æ —Ñ–∞–π–ª –±“Ø—Ä–∏–π–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö
        for audio_file in audio_files:
            try:
                # –ê—É–¥–∏–æ –∞—á–∞–∞–ª–∞—Ö
                audio, sr = librosa.load(audio_file, sr=CONFIG['sr'])
                
                # –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö
                features = extract_advanced_features(audio, sr)
                
                if features is not None:
                    X.append(features.flatten())  # 1D –≤–µ–∫—Ç–æ—Ä –±–æ–ª–≥–æ—Ö
                    y.append(emotion)  # Label –Ω—ç–º—ç—Ö
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è {audio_file.name} - –ê–ª–¥–∞–∞: {e}")
    
    return np.array(X), np.array(y), sorted(set(emotions))

# =====================================================
#  –ñ–ò–®–≠–≠ ”®–ì”®–ì–î”®–õ “Æ“Æ–°–ì–≠–• (–¢–ï–°–¢ –ó–û–†–ò–õ–ì–û–û–†)
# =====================================================
def create_synthetic_dataset(n_samples=200):
    """
    –ñ–∏—à—ç—ç ”©–≥”©–≥–¥”©–ª “Ø“Ø—Å–≥—ç—Ö (–±–æ–¥–∏—Ç —Å–∏—Å—Ç–µ–º–¥ “Ø“Ø–Ω–∏–π–≥ —Å–æ–ª–∏—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π!)
    
    –ó”©–≤—Ö”©–Ω —Ç–µ—Å—Ç–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ–æ—Ä —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π –¥—É—É “Ø“Ø—Å–≥—ç–∂,
    —Ç“Ø“Ø–Ω–¥ —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω label ”©–≥–Ω”©.
    
    –ë–æ–¥–∏—Ç —Å–∏—Å—Ç–µ–º–¥ —ç–Ω—ç —Ö—ç—Å–≥–∏–π–≥ –ë–û–î–ò–¢ –î–£–£ –•–û–û–õ–û–ô–ù ”©–≥”©–≥–¥–ª”©”©—Ä —Å–æ–ª–∏—Ö —ë—Å—Ç–æ–π!
    
    Parameters:
        n_samples: –ù–∏–π—Ç “Ø“Ø—Å–≥—ç—Ö ”©–≥”©–≥–¥–ª–∏–π–Ω —Ç–æ–æ
        
    Returns:
        X, y, emotions: prepare_dataset() —Ñ—É–Ω–∫—Ü—Ç—ç–π –∞–¥–∏–ª
    """
    print("üîß –ñ–∏—à—ç—ç ”©–≥”©–≥–¥”©–ª “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞ (–±–æ–¥–∏—Ç —Å–∏—Å—Ç–µ–º–¥ “Ø“Ø–Ω–∏–π–≥ —Å–æ–ª–∏—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π)...")
    
    emotions = ['happy', 'sad', 'angry', 'neutral', 'fear', 'surprise']
    n_per_emotion = n_samples // len(emotions)  # –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª –±“Ø—Ä—Ç –∏–∂–∏–ª —Ç–æ–æ
    
    X = []
    y = []
    
    # –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª –±“Ø—Ä—ç—ç—Ä –¥–∞–≤—Ç–∞—Ö
    for emotion in emotions:
        for i in range(n_per_emotion):
            # –°–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π —É—Ä—Ç—Ç–∞–π –¥—É—É
            duration = np.random.uniform(2, 4)
            audio = np.random.randn(int(duration * CONFIG['sr'])) * 0.1
            
            # –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª —Ç—É—Å –±“Ø—Ä—Ç ”©”©—Ä –¥–∞–≤—Ç–∞–º–∂ ”©–≥”©—Ö (–∂–∏—à—ç—ç –ª)
            if emotion == 'happy':
                # –ë–∞—è—Ä —Ö”©”©—Ä—Ç—ç–π –¥—É—É = ”©–Ω–¥”©—Ä –¥–∞–≤—Ç–∞–º–∂ (440 Hz)
                audio = audio * np.sin(2 * np.pi * 440 * np.arange(len(audio)) / CONFIG['sr'])
            elif emotion == 'angry':
                # –£—É—Ä—Ç–∞–π –¥—É—É = –Ω–∞–º—Ö–∞–Ω –¥–∞–≤—Ç–∞–º–∂ (200 Hz)
                audio = audio * np.sin(2 * np.pi * 200 * np.arange(len(audio)) / CONFIG['sr'])
            elif emotion == 'sad':
                # –ì—É–Ω–∏–≥—Ç–∞–π –¥—É—É = –º–∞—à –Ω–∞–º—Ö–∞–Ω –¥–∞–≤—Ç–∞–º–∂ (150 Hz)
                audio = audio * np.sin(2 * np.pi * 150 * np.arange(len(audio)) / CONFIG['sr'])
            
            # –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö
            features = extract_advanced_features(audio)
            
            if features is not None:
                X.append(features.flatten())
                y.append(emotion)
    
    print(f"‚úÖ {len(X)} –∂–∏—à—ç—ç ”©–≥”©–≥–¥”©–ª “Ø“Ø—Å–≥—ç–ª—ç—ç\n")
    return np.array(X), np.array(y), emotions

# =====================================================
#  –ó–ê–ì–í–ê–† –°–£–†–ì–ê–•
# =====================================================
def train_model(X, y, emotions):
    """
    –î—ç–≤—à–∏–ª—Ç—ç—Ç RandomForest –∑–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞—Ö
    
    RandomForest: –û–ª–æ–Ω –º–æ–¥—ã–≥ (decision trees) —Ö–æ—Å–ª—É—É–ª–∂,
    –∏–ª“Ø“Ø –Ω–∞–π–¥–≤–∞—Ä—Ç–∞–π —Ç–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞–¥–∞–≥ –∞–ª–≥–æ—Ä–∏—Ç–º
    
    Parameters:
        X: –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–Ω –º–∞—Å—Å–∏–≤
        y: Label (—Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥)
        emotions: –ë“Ø—Ö –±–æ–ª–æ–º–∂–∏—Ç —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥
        
    Returns:
        model: –°—É—Ä–≥–∞—Å–∞–Ω –∑–∞–≥–≤–∞—Ä
        scaler: –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–≥—á
    """
    print("üå≤ –ó–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
    
    # ”®–≥”©–≥–¥–ª–∏–π–≥ —Å—É—Ä–≥–∞–ª—Ç –±–∞ —Ç–µ—Å—Ç –≥—ç–∂ —Ö—É–≤–∞–∞—Ö (80% vs 20%)
    # stratify=y: label –±“Ø—Ä –ø—Ä–æ–ø–æ—Ä—Ü —Ö–∞–¥–≥–∞–ª–Ω–∞ (–∂–∏—à—ç—ç –Ω—å happy –¥—É—É–Ω—É—É–¥ 80-20 —Ö—É–≤–∞–∞–≥–¥–∞–Ω–∞)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–ª (Feature scaling)
    # ”®–≥”©–≥–¥–ª–∏–π–≥ –¥—É–Ω–¥–∞–∂=0, —Ö–∞–∑–∞–π–ª—Ç=1 –±–æ–ª–≥–æ—Ö
    # –≠–Ω—ç –Ω—å –∑–∞–≥–≤–∞—Ä—ã–≥ –∏–ª“Ø“Ø —Ö—É—Ä–¥–∞–Ω —Å—É—Ä–∞–ª—Ü–∞—Ö–∞–¥ —Ç—É—Å–∞–ª–Ω–∞
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)  # –°—É—Ä–≥–∞–ª—Ç –¥—ç—ç—Ä —Å—É—Ä–∞–ª—Ü–∞—Ö
    X_test_scaled = scaler.transform(X_test)  # –¢–µ—Å—Ç –¥—ç—ç—Ä –∑”©–≤—Ö”©–Ω —Ö—ç—Ä—ç–≥–ª—ç—Ö
    
    # RandomForest –∑–∞–≥–≤–∞—Ä “Ø“Ø—Å–≥—ç—Ö
    model = RandomForestClassifier(
        n_estimators=300,      # 300 –º–æ–¥ –∞—à–∏–≥–ª–∞—Ö (–∏–ª“Ø“Ø –∏—Ö = –∏–ª“Ø“Ø –Ω–∞–π–¥–≤–∞—Ä—Ç–∞–π)
        max_depth=20,          # –ú–æ–¥–Ω—ã —Ö–∞–º–≥–∏–π–Ω –∏—Ö –≥“Ø–Ω (overfit-—ç—ç—Å —Å—ç—Ä–≥–∏–π–ª–Ω—ç)
        min_samples_split=5,   # Node —Ö—É–≤–∞–∞—Ö–∞–¥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π —Ö–∞–º–≥–∏–π–Ω –±–∞–≥–∞ —Å—ç–º–ø–ª
        min_samples_leaf=2,    # –ù–∞–≤—á node-–¥ –±–∞–π—Ö —ë—Å—Ç–æ–π —Ö–∞–º–≥–∏–π–Ω –±–∞–≥–∞ —Å—ç–º–ø–ª
        random_state=42,       # –î–∞–≤—Ç–∞–≥–¥–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π “Ø—Ä –¥“Ø–Ω
        n_jobs=-1,             # –ë“Ø—Ö CPU –∞—à–∏–≥–ª–∞—Ö
        class_weight='balanced'  # Label –±“Ø—Ä–∏–π–≥ —Ç—ç–Ω—Ü“Ø“Ø –∂–∏–Ω–ª—ç—Ö
    )
    
    # –ó–∞–≥–≤–∞—Ä—ã–≥ —Å—É—Ä–≥–∞—Ö
    model.fit(X_train_scaled, y_train)
    
    # “Æ–Ω—ç–ª–≥—ç—ç —Ö–∏–π—Ö
    y_pred = model.predict(X_test_scaled)  # –¢–µ—Å—Ç –¥—ç—ç—Ä —Ç–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö
    accuracy = model.score(X_test_scaled, y_test)  # –ù–∞—Ä–∏–π–≤—á–ª–∞–ª —Ç–æ–æ—Ü–æ—Ö
    
    # “Æ—Ä –¥“Ø–Ω–≥ —Ö—ç–≤–ª—ç—Ö
    print(f"\nüìà –ó–∞–≥–≤–∞—Ä—ã–Ω “Ø–Ω—ç–ª–≥—ç—ç:")
    print(f"   –ù–∞—Ä–∏–π–≤—á–ª–∞–ª: {accuracy * 100:.2f}%\n")
    print("üìä –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç–∞–π–ª–∞–Ω:")
    # Precision, Recall, F1-score –∑—ç—Ä–≥–∏–π–≥ label –±“Ø—Ä—ç—ç—Ä —Ö–∞—Ä—É—É–ª–Ω–∞
    print(classification_report(y_test, y_pred, target_names=emotions))
    
    # Confusion matrix —Ö–∞—Ä—É—É–ª–∞—Ö (–∞–ª–¥–∞–∞ —Ö–∞–∞–Ω–∞ –±–∞–π–≥–∞–∞–≥ —Ö–∞—Ä–∞—Ö)
    plot_confusion_matrix(y_test, y_pred, emotions)
    
    # –ó–∞–≥–≤–∞—Ä—ã–≥ —Ñ–∞–π–ª –±–æ–ª–≥–æ–Ω —Ö–∞–¥–≥–∞–ª–∞—Ö (–¥–∞—Ä–∞–∞ –¥–∞—Ö–∏–Ω –∞—à–∏–≥–ª–∞—Ö—ã–Ω —Ç—É–ª–¥)
    joblib.dump(model, CONFIG['model_path'])
    joblib.dump(scaler, CONFIG['scaler_path'])
    
    print(f"üíæ –ó–∞–≥–≤–∞—Ä —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: {CONFIG['model_path']}\n")
    
    return model, scaler

# =====================================================
#  CONFUSION MATRIX –•–ê–†–£–£–õ–ê–•
# =====================================================
def plot_confusion_matrix(y_true, y_pred, emotions):
    """
    Confusion matrix-–∏–π–≥ –≥—Ä–∞—Ñ–∏–∫ –±–æ–ª–≥–æ—Ö
    
    Confusion matrix: –ó–∞–≥–≤–∞—Ä —è–º–∞—Ä label-—É—É–¥—ã–≥ –∞–ª–¥—Å–∞–Ω—ã–≥ —Ö–∞—Ä—É—É–ª–Ω–∞
    –ñ–∏—à—ç—ç –Ω—å: "happy" –≥—ç–∂ –±–æ–¥—Å–æ–Ω –±–æ–ª–æ–≤—á “Ø–Ω—ç–Ω–¥—ç—ç "sad" –±–∞–π—Å–∞–Ω
    
    Parameters:
        y_true: –ë–æ–¥–∏—Ç label-—É—É–¥
        y_pred: –¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω label-—É—É–¥
        emotions: Label-—É—É–¥—ã–Ω –Ω—ç—Ä—Å
    """
    # Confusion matrix —Ç–æ–æ—Ü–æ—Ö
    cm = confusion_matrix(y_true, y_pred, labels=emotions)
    
    # –ì—Ä–∞—Ñ–∏–∫ “Ø“Ø—Å–≥—ç—Ö
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=emotions, yticklabels=emotions)
    plt.title('Confusion Matrix - –ê–ª–¥–∞–∞–Ω—ã –º–∞—Ç—Ä–∏—Ü')
    plt.ylabel('–ë–æ–¥–∏—Ç —É—Ç–≥–∞')
    plt.xlabel('–¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥–∞')
    plt.tight_layout()
    
    # –§–∞–π–ª –±–æ–ª–≥–æ–Ω —Ö–∞–¥–≥–∞–ª–∞—Ö
    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
    print("üíæ Confusion matrix —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: confusion_matrix.png")

# =====================================================
#  –ó–ê–ì–í–ê–† –ê–ß–ê–ê–õ–ê–•
# =====================================================
def load_model():
    """
    –•–∞–¥–≥–∞–ª—Å–∞–Ω –∑–∞–≥–≤–∞—Ä—ã–≥ –∞—á–∞–∞–ª–∞—Ö
    
    –•—ç—Ä—ç–≤ –∑–∞–≥–≤–∞—Ä –±–∞–π—Ö–≥“Ø–π –±–æ–ª —à–∏–Ω—ç—ç—Ä —Å—É—Ä–≥–∞–Ω–∞.
    
    Returns:
        model: –ê—á–∞–∞–ª—Å–∞–Ω –∑–∞–≥–≤–∞—Ä
        scaler: –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–≥—á
        emotions: –ë–æ–ª–æ–º–∂–∏—Ç —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥
    """
    # –ó–∞–≥–≤–∞—Ä –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞—Ö
    if not os.path.exists(CONFIG['model_path']):
        print("‚ö†Ô∏è  –ó–∞–≥–≤–∞—Ä –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –®–∏–Ω—ç –∑–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\n")
        X, y, emotions = prepare_dataset()
        model, scaler = train_model(X, y, emotions)
        return model, scaler, emotions
    
    # –ó–∞–≥–≤–∞—Ä—ã–≥ –∞—á–∞–∞–ª–∞—Ö
    model = joblib.load(CONFIG['model_path'])
    scaler = joblib.load(CONFIG['scaler_path'])
    emotions = model.classes_  # –ó–∞–≥–≤–∞—Ä—ã–Ω —Å—É—Ä–∞–ª—Ü—Å–∞–Ω label-—É—É–¥
    
    print(f"‚úÖ –ó–∞–≥–≤–∞—Ä –∞–º–∂–∏–ª—Ç—Ç–∞–π –∞—á–∞–∞–ª–∞–≥–¥–ª–∞–∞!")
    print(f"   –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥: {', '.join(emotions)}\n")
    
    return model, scaler, emotions

# =====================================================
#  –¢–ê–ê–ú–ê–ì –ì–ê–†–ì–ê–•
# =====================================================
def predict_emotion(model, scaler, features):
    """
    –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª —Ç–∞–∞—Ö
    
    Parameters:
        model: –°—É—Ä–≥–∞—Å–∞–Ω –∑–∞–≥–≤–∞—Ä
        scaler: –°—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞–≥—á
        features: –î—É—É–Ω—ã —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥
        
    Returns:
        emotions: –ë“Ø—Ö –±–æ–ª–æ–º–∂–∏—Ç —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥
        probabilities: –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª –±“Ø—Ä–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª (0-1)
        best_emotion: –•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä –º–∞–≥–∞–¥–ª–∞–ª—Ç–∞–π —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª
    """
    if features is None:
        return None, None, None
    
    # –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—ã–≥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—á–∏–ª–∞—Ö (—Å—É—Ä–≥–∞–ª—Ç—Ç–∞–π –∞–¥–∏–ª—Ö–∞–Ω –±–∞–π—Ö —ë—Å—Ç–æ–π)
    features_scaled = scaler.transform(features)
    
    # –ú–∞–≥–∞–¥–ª–∞–ª —Ç–æ–æ—Ü–æ—Ö (label –±“Ø—Ä–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª—ã–≥ –æ–ª–Ω–æ)
    probabilities = model.predict_proba(features_scaled)[0]
    
    # Label-—É—É–¥—ã–≥ –∞–≤–∞—Ö
    emotions = model.classes_
    
    # –•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä –º–∞–≥–∞–¥–ª–∞–ª—Ç–∞–π label
    best_emotion = emotions[np.argmax(probabilities)]
    
    return emotions, probabilities, best_emotion

# =====================================================
#  –í–ò–ó–£–ê–õ “Æ–† –î“Æ–ù
# =====================================================
def plot_emotion_results(emotions, probabilities, audio, sr):
    """
    “Æ—Ä –¥“Ø–Ω–≥ –≥—Ä–∞—Ñ–∏–∫ —Ö—ç–ª–±—ç—Ä—ç—ç—Ä —Ö–∞—Ä—É—É–ª–∞—Ö
    
    4 –≥—Ä–∞—Ñ–∏–∫ —Ö–∞—Ä—É—É–ª–Ω–∞:
    1. –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª (bar chart)
    2. –î—É—É —Ö–æ–æ–ª–æ–π–Ω –¥–æ–ª–≥–∏–æ–Ω (waveform)
    3. Mel Spectrogram (–¥–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö—É–≤–∞–∞—Ä—å)
    4. MFCC —à–∏–Ω–∂ —á–∞–Ω–∞—Ä
    
    Parameters:
        emotions: –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª“Ø“Ø–¥
        probabilities: –ú–∞–≥–∞–¥–ª–∞–ª—É—É–¥
        audio: –î—É—É —Ö–æ–æ–ª–æ–π
        sr: Sampling rate
    """
    # 2x2 —Ö—ç–º–∂—ç—ç—Ç—ç–π –≥—Ä–∞—Ñ–∏–∫ “Ø“Ø—Å–≥—ç—Ö
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # ===== 1. –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª =====
    ax1 = axes[0, 0]
    # ”®–Ω–≥”©: –º–∞–≥–∞–¥–ª–∞–ª –∏—Ö—Å—ç—Ö —Ç—É—Å–∞–º –≥—ç—Ä—ç–ª
    colors = plt.cm.viridis(probabilities / probabilities.max())
    bars = ax1.bar(emotions, probabilities, color=colors, edgecolor='black', linewidth=1.5)
    ax1.set_title('üé≠ –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª', fontsize=14, fontweight='bold')
    ax1.set_ylabel('–ú–∞–≥–∞–¥–ª–∞–ª', fontsize=12)
    ax1.set_ylim(0, 1)  # Y —Ç—ç–Ω—Ö–ª—ç–≥ 0-1 —Ö“Ø—Ä—Ç—ç–ª
    ax1.grid(axis='y', linestyle='--', alpha=0.3)  # –•—ç–≤—Ç—ç—ç —à—É–≥–∞–º
    
    # –•–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä –º–∞–≥–∞–¥–ª–∞–ª—Ç–∞–π bar-—ã–≥ —É–ª–∞–∞–Ω–∞–∞—Ä —Ç—ç–º–¥—ç–≥–ª—ç—Ö
    max_idx = np.argmax(probabilities)
    bars[max_idx].set_edgecolor('red')
    bars[max_idx].set_linewidth(3)
    
    # Bar –±“Ø—Ä–∏–π–Ω –¥—ç—ç—Ä —Ö—É–≤–∏–π–≥ –±–∏—á–∏—Ö
    for i, (bar, prob) in enumerate(zip(bars, probabilities)):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{prob*100:.1f}%', ha='center', va='bottom', fontsize=10)
    
    # ===== 2. –ê—É–¥–∏–æ –¥–æ–ª–≥–∏–æ–Ω =====
    ax2 = axes[0, 1]
    # –¶–∞–≥ —Ö—É–≥–∞—Ü–∞–∞–Ω—ã —Ç—ç–Ω—Ö–ª—ç–≥ –±—ç–ª—Ç–≥—ç—Ö
    time = np.arange(len(audio)) / sr
    ax2.plot(time, audio, color='steelblue', linewidth=0.5)
    ax2.set_title('üåä –î—É—É —Ö–æ–æ–ª–æ–π–Ω –¥–æ–ª–≥–∏–æ–Ω', fontsize=14, fontweight='bold')
    ax2.set_xlabel('–•—É–≥–∞—Ü–∞–∞ (—Å–µ–∫—É–Ω–¥)', fontsize=12)
    ax2.set_ylabel('–ê–º–ø–ª–∏—Ç—É–¥', fontsize=12)
    ax2.grid(True, alpha=0.3)
    
    # ===== 3. Mel Spectrogram =====
    ax3 = axes[1, 0]
    # Mel Spectrogram —Ç–æ–æ—Ü–æ—Ö (–¥–∞–≤—Ç–∞–º–∂–∏–π–Ω —Ö—É–≤–∞–∞—Ä—å)
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)
    # Power-—ã–≥ –¥–µ—Ü–∏–±–µ–ª –±–æ–ª–≥–æ—Ö (—Ö–∞—Ä–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    # Spectrogram-—ã–≥ —Ö–∞—Ä—É—É–ª–∞—Ö
    img = librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', 
                                     ax=ax3, cmap='magma')
    ax3.set_title('üéµ Mel Spectrogram', fontsize=14, fontweight='bold')
    plt.colorbar(img, ax=ax3, format='%+2.0f dB')  # Colorbar –Ω—ç–º—ç—Ö
    
    # ===== 4. MFCC =====
    ax4 = axes[1, 1]
    # MFCC —Ç–æ–æ—Ü–æ—Ö (—ç—Ö–Ω–∏–π 20 –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
    # MFCC-–∏–π–≥ —Ö–∞—Ä—É—É–ª–∞—Ö
    img2 = librosa.display.specshow(mfcc, sr=sr, x_axis='time', ax=ax4, cmap='coolwarm')
    ax4.set_title('üìä MFCC –®–∏–Ω–∂ —á–∞–Ω–∞—Ä', fontsize=14, fontweight='bold')
    plt.colorbar(img2, ax=ax4)
    
    # –ë“Ø—Ö –≥—Ä–∞—Ñ–∏–∫—É—É–¥—ã–≥ –∑–æ—Ö–∏—Ü—É—É–ª–∞—Ö
    plt.tight_layout()
    
    # –§–∞–π–ª –±–æ–ª–≥–æ–Ω —Ö–∞–¥–≥–∞–ª–∞—Ö
    plt.savefig('emotion_analysis.png', dpi=150, bbox_inches='tight')
    print("üíæ –ì—Ä–∞—Ñ–∏–∫ —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: emotion_analysis.png")
    plt.show()  # –ì—Ä–∞—Ñ–∏–∫ —Ö–∞—Ä—É—É–ª–∞—Ö

# =====================================================
#  “Æ–ù–î–°–≠–ù –ì“Æ–ô–¶–≠–¢–ì–≠–ì–ß
# =====================================================
def main():
    """
    “Æ–Ω–¥—Å—ç–Ω –ø—Ä–æ–≥—Ä–∞–º - –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    
    3 “Ø–Ω–¥—Å—ç–Ω “Ø–π–ª–¥—ç–ª:
    1. –î—É—É –±–∏—á–∏–∂ —Ç–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö - –ú–∏–∫—Ä–æ—Ñ–æ–Ω–æ–æ—Ä –¥—É—É –±–∏—á–∏–∂ —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö
    2. –ó–∞–≥–≤–∞—Ä—ã–≥ –¥–∞—Ö–∏–Ω —Å—É—Ä–≥–∞—Ö - ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥–∞–∞—Å —à–∏–Ω—ç—ç—Ä —Å—É—Ä–∞–ª—Ü–∞—Ö
    3. –ì–∞—Ä–∞—Ö - –ü—Ä–æ–≥—Ä–∞–º—ã–≥ –¥—É—É—Å–≥–∞—Ö
    """
    # –ì–∞—Ä—á–∏–≥ —Ö—ç–≤–ª—ç—Ö
    print("\n" + "="*60)
    print("üé§ –î–£–£–ù–î –°–≠–¢–ì–≠–õ –•”®–î–õ”®–õ –¢–ê–ù–ò–• –°–ò–°–¢–ï–ú".center(60))
    print("="*60 + "\n")
    
    # –ó–∞–≥–≤–∞—Ä –∞—á–∞–∞–ª–∞—Ö (—ç—Å–≤—ç–ª —à–∏–Ω—ç—ç—Ä —Å—É—Ä–≥–∞—Ö)
    model, scaler, emotions = load_model()
    
    # “Æ–Ω–¥—Å—ç–Ω –¥–∞–≤—Ç–∞–ª—Ç (—Ö—ç—Ä—ç–≥–ª—ç–≥—á –≥–∞—Ä–∞—Ö —Ö“Ø—Ä—Ç—ç–ª)
    while True:
        # –¶—ç—Å —Ö–∞—Ä—É—É–ª–∞—Ö
        print("\nüìã –°–æ–Ω–≥–æ–ª—Ç—É—É–¥:")
        print("   1. –î—É—É –±–∏—á–∏–∂ —Ç–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö")
        print("   2. –ó–∞–≥–≤–∞—Ä—ã–≥ –¥–∞—Ö–∏–Ω —Å—É—Ä–≥–∞—Ö")
        print("   3. –ì–∞—Ä–∞—Ö")
        
        # –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Å–æ–Ω–≥–æ–ª—Ç –∞–≤–∞—Ö
        choice = input("\nüëâ –°–æ–Ω–≥–æ–ª—Ç –æ—Ä—É—É–ª–Ω–∞ —É—É (1-3): ").strip()
        
        # ===== –°–û–ù–ì–û–õ–¢ 1: –î—É—É –±–∏—á–∏–∂ —Ç–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö =====
        if choice == '1':
            # –î—É—É –±–∏—á–∏—Ö
            audio = record_audio()
            
            # –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞—Ö
            print("üîç –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥—ã–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∂ –±–∞–π–Ω–∞...")
            features = extract_advanced_features(audio)
            
            # –•—ç—Ä—ç–≤ —à–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞–∂ —á–∞–¥–∞–∞–≥“Ø–π –±–æ–ª
            if features is None:
                print("‚ùå –®–∏–Ω–∂ —á–∞–Ω–∞—Ä –≥–∞—Ä–≥–∞–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π. –î–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É.")
                continue
            
            # –¢–∞–∞–º–∞–≥ –≥–∞—Ä–≥–∞—Ö
            print("ü§ñ –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–∂ –±–∞–π–Ω–∞...\n")
            emo_list, probs, best = predict_emotion(model, scaler, features)
            
            # ===== “Æ—Ä –¥“Ø–Ω —Ö–∞—Ä—É—É–ª–∞—Ö =====
            print("="*60)
            print("üìä “Æ–† –î“Æ–ù".center(60))
            print("="*60)
            
            # –°—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª –±“Ø—Ä–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª—ã–≥ —Ö–∞—Ä—É—É–ª–∞—Ö
            for emo, prob in zip(emo_list, probs):
                # Progress bar “Ø“Ø—Å–≥—ç—Ö
                bar_length = int(prob * 40)  # 40 —Ç—ç–º–¥—ç–≥—Ç–∏–π–Ω —É—Ä—Ç bar
                bar = '‚ñà' * bar_length + '‚ñë' * (40 - bar_length)
                # –§–æ—Ä–º–∞—Ç–ª–∞–∂ —Ö—ç–≤–ª—ç—Ö
                print(f"   {emo.upper():<12} {bar} {prob*100:>6.2f}%")
            
            # –î“Ø–≥–Ω—ç–ª—Ç —Ö—ç–≤–ª—ç—Ö
            print("\n" + "="*60)
            print(f"üéØ –î“Æ–ì–ù–≠–õ–¢: {best.upper()} —Å—ç—Ç–≥—ç–ª —Ö”©–¥–ª”©–ª –¥–∞–≤–∞–º–≥–∞–π –±–∞–π–Ω–∞!".center(60))
            print("="*60 + "\n")
            
            # –ì—Ä–∞—Ñ–∏–∫ “Ø“Ø—Å–≥—ç—Ö —ç—Å—ç—Ö–∏–π–≥ –∞—Å—É—É—Ö
            response = input("üìà –ì—Ä–∞—Ñ–∏–∫ —Ö–∞—Ä—É—É–ª–∞—Ö —É—É? (y/n): ").strip().lower()
            if response == 'y':
                plot_emotion_results(emo_list, probs, audio, CONFIG['sr'])
        
        # ===== –°–û–ù–ì–û–õ–¢ 2: –ó–∞–≥–≤–∞—Ä –¥–∞—Ö–∏–Ω —Å—É—Ä–≥–∞—Ö =====
        elif choice == '2':
            print("\nüîÑ –ó–∞–≥–≤–∞—Ä –¥–∞—Ö–∏–Ω —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\n")
            # ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω –∞—á–∞–∞–ª–∞—Ö
            X, y, emotions = prepare_dataset()
            # –ó–∞–≥–≤–∞—Ä —Å—É—Ä–≥–∞—Ö
            model, scaler = train_model(X, y, emotions)
            print("‚úÖ –ó–∞–≥–≤–∞—Ä –∞–º–∂–∏–ª—Ç—Ç–∞–π —Å—É—Ä–≥–∞–≥–¥–ª–∞–∞!\n")
        
        # ===== –°–û–ù–ì–û–õ–¢ 3: –ì–∞—Ä–∞—Ö =====
        elif choice == '3':
            print("\nüëã –ë–∞—è—Ä—Ç–∞–π! –î–∞—Ö–∏–Ω —É—É–ª–∑–∞—Ü–≥–∞–∞—è!\n")
            break  # –î–∞–≤—Ç–∞–ª—Ç–∞–∞—Å –≥–∞—Ä–∞—Ö
        
        # ===== –ë—É—Ä—É—É —Å–æ–Ω–≥–æ–ª—Ç =====
        else:
            print("‚ùå –ë—É—Ä—É—É —Å–æ–Ω–≥–æ–ª—Ç. 1-3 —Ç–æ–æ –æ—Ä—É—É–ª–Ω–∞ —É—É.")

# =====================================================
#  –ü–†–û–ì–†–ê–ú –≠–•–õ“Æ“Æ–õ–≠–•
# =====================================================
if __name__ == "__main__":
    """
    –≠–Ω—ç –Ω—å Python-–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –∫–æ–¥–ª–æ—Ö —Ö—ç–≤ –º–∞—è–≥.
    
    __name__ == "__main__" –≥—ç–¥—ç–≥ –Ω—å:
    - –•—ç—Ä—ç–≤ —ç–Ω—ç —Ñ–∞–π–ª—ã–≥ —à—É—É–¥ –∞–∂–∏–ª–ª—É—É–ª–±–∞–ª (python script.py) True –±–∞–π–Ω–∞
    - –•—ç—Ä—ç–≤ ”©”©—Ä —Ñ–∞–π–ª–∞–∞—Å import —Ö–∏–π–≤—ç–ª False –±–∞–π–Ω–∞
    
    –ò–Ω–≥—ç—Å–Ω—ç—ç—Ä —ç–Ω—ç —Ñ–∞–π–ª—ã–≥ –º–æ–¥—É–ª—å –±–æ–ª–≥–æ–Ω –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–Ω–æ:
    from script import extract_advanced_features
    """
    main()  # “Æ–Ω–¥—Å—ç–Ω —Ñ—É–Ω–∫—Ü–∏–π–≥ –¥—É—É–¥–∞—Ö